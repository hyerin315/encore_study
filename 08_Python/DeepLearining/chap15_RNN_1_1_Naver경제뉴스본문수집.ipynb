{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import date,datetime, timedelta\n",
    "import re, os, requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Edg/86.0.622.38'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스기사 주소 수집\n",
    "def naver_news(start_date = None, end_date = None):\n",
    "    sid2_cate = [259, 258, 261, 771, 260, 310, 263]\n",
    "    url = \"https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid2={}&sid1=101&date={}&page={}\"\n",
    "    head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Edg/86.0.622.38'}\n",
    "    \n",
    "    for date_ in [str(x).replace(\"-\", \"\")[:8] for x in pd.date_range(start_date, end_date)]: #pd.date_range : 기간 생성(workingday로 주말 뺄 수도 있음)\n",
    "        for cate in sid2_cate:\n",
    "            r= requests.get(url.format(cate, date_, 200), headers= head) #200페이지를 썼을 때, 오류가 발생하지 않고 맨 마지막 페이지로 감\n",
    "            bs = BeautifulSoup(r.text, \"lxml\")\n",
    "            #print(x , end=' : ')\n",
    "            \n",
    "            #최종 페이지를 들고옴\n",
    "            last_page = bs.find(\"div\", class_='paging').find(\"strong\").text\n",
    "            \n",
    "            #print (\"last page:{}\".format(last_page)\n",
    "            for page_num in range(1, int(last_page)+1):\n",
    "                r2= requests.get(url.format(cate, date_, page_num ), headers= head)\n",
    "                bs2 = BeautifulSoup(r2.text, 'lxml')\n",
    "                \n",
    "                for x in bs2.find(\"div\", class_=\"list_body newsflash_body\").findAll(\"dt\"):\n",
    "                    article(date_, x.a['href']) #dt에 있는 a태그의 'href'를 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규식을 사용해 필요없는 정보 삭제\n",
    "def article(date_, url):\n",
    "    reporter = re.compile(\"[가-힣]{2,4}\\s*기자\")\n",
    "    email   = re.compile(\"[\\w._%+-]+@[\\w.-]+\\.[a-zA-Z]{2,4}\")\n",
    "    r = requests.get(url, headers= head)\n",
    "    bs = BeautifulSoup(r.text)\n",
    "    try:\n",
    "        rt = bs.find(\"div\", id=\"articleBodyContents\")\n",
    "        text = rt.text.strip()\n",
    "    except:\n",
    "        return  \n",
    "    \n",
    "    try:\n",
    "        text = text[:text.find(rt.select_one(\"a\").text)]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #정규식으로 email, reporter 삭제함 \n",
    "    text = re.sub(email, \"\", text)\n",
    "    text = re.sub(reporter, \"\", text)\n",
    "    \n",
    "    #폴더 생성\n",
    "    if not os.path.isdir(date_):\n",
    "        os.mkdir(date_)\n",
    "    \n",
    "    f = open(date_ + \"/\" + url.split(\"?\")[-1] + \".txt\", \"w\", encoding='utf-8')\n",
    "    f.write(text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_news('2021-05-25', '2021-05-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
